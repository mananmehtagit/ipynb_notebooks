{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMJBLHx8R70pkXSmR/nnRW/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mananmehtagit/ipynb_notebooks/blob/main/ai_agent_topic_explorer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose of this agent is scrape web for relevent articles and information based on input topic\n",
        "\n",
        "We will create two tools the first tool is for searching the internet and getting URLs, and the second tool is for reading those URLs.\n"
      ],
      "metadata": {
        "id": "KFLtTTcPyz_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -qU langchain-google-vertexai # use this for commercial use\n",
        "\n",
        "!pip install langchain_google_genai duckduckgo_search #use google genai for prototype purpose\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "collapsed": true,
        "id": "m4dj4WDsB6EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: the first tool is for searching the internet and getting URLs, and the second tool is for reading those URLs.\n",
        "\n",
        "from duckduckgo_search import DDGS\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def search_ddgs_and_store(query, num_results=10):\n",
        "  \"\"\"Searches DuckDuckGo and stores results in an array.\n",
        "\n",
        "  Args:\n",
        "    query: The search query.\n",
        "    num_results: The maximum number of results to retrieve.\n",
        "\n",
        "  Returns:\n",
        "    An array of search result dictionaries.\n",
        "  \"\"\"\n",
        "  search_results = []\n",
        "  results = DDGS().text(query, region='wt-wt', safesearch='Moderate', timelimit='y', max_results=num_results)  # fetch results from DDGS\n",
        "\n",
        "  # Iterate through the results and extract relevant information\n",
        "  for i,result in enumerate(results): # extract title, snippet, and URL\n",
        "        search_results.append(result)\n",
        "\n",
        "  return search_results\n",
        "\n",
        "\n",
        "\n",
        "# summarize text from url\n",
        "def summarize_url(url):\n",
        "  \"\"\"Reads a URL, fetches content, and creates a summary using Google AI LLM.\n",
        "\n",
        "  Args:\n",
        "    url: The URL to process.\n",
        "\n",
        "  Returns:\n",
        "    A summary of the URL content.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    # Fetch the content of the URL\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    # Parse the HTML content using Beautiful Soup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Extract the text content\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Initialize the Gemini model\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "    # Generate the summary\n",
        "    summary = llm.predict(f\"Summarize the following text: {text}\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching URL {url}: {e}\")\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"Error processing URL {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Example usage\n",
        "topic = \"What is the latest news on AI safety research?\"\n",
        "urls = search_ddgs_and_store(topic,3)  # Get the top 3 URLs\n",
        "# print(urls)\n",
        "\n",
        "#print summary of each url and store in array\n",
        "summaries = []\n",
        "for url in urls:\n",
        "  # print(summarize_url(url['href']))\n",
        "  summary = summarize_url(url['href'])\n",
        "  if summary:\n",
        "    summaries.append(summary)\n",
        "\n",
        "for index, summary in enumerate(summaries):\n",
        "  # print(f\"{index}: {urls[index]['title']}\")\n",
        "  # print(f\"Summary {index}: {summary}\")\n",
        "  # print title of given index and correspoind summary\n",
        "   print(f\"Summary for Title {urls[index]['title']}: (Link:{urls[index]['href']})\")\n",
        "   print(summary)\n",
        "   print(\"-\" * 20)\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GGjenc9dCfZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cMhrvuK4K-0r"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HBGfg-LDQrGh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}